{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import regex\n",
    "from pprint import pformat\n",
    "\n",
    "path = Path(\n",
    "    \"/Users/jonathan/mres_thesis/wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/notes/devnotes.md\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_notes_from_path(path) -> list[str]:\n",
    "    with open(path, \"r\") as f:\n",
    "        string = f.read()\n",
    "\n",
    "    pattern = r\"(?<=\\])\\n\\n(?=\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    notes = regex.split(pattern=pattern, string=string)\n",
    "\n",
    "    # simple validation of the regex split. All notes should start with a '2' and end with a ']'\n",
    "    for note in notes:\n",
    "        if note[0] != \"2\":\n",
    "            raise ValueError(\"note doesnt start with a 2\")\n",
    "        if note[-1] != \"]\":\n",
    "            raise ValueError(\"note doesnt end with a ']'\")\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "notes = get_notes_from_path(path=path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_note_fields(note: str) -> dict[str, str] | None:\n",
    "    regexps = dict(\n",
    "        datetime=r\"(?P<datetime>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\",\n",
    "        separator=r\" - \",\n",
    "        title=r\"(?P<title>.+?(?=\\.|\\?))\",\n",
    "        content=r\"(?<=\\2)\\. (?P<content>.+?)(?= tag)\",\n",
    "        tags=r\"(?<=\\3) tags: (?P<tags>.*)$\",\n",
    "    )\n",
    "\n",
    "    regexp = \"\"\n",
    "    for patt in regexps.values():\n",
    "        regexp += patt\n",
    "\n",
    "    match = regex.match(regexp, note)\n",
    "\n",
    "    if match:\n",
    "        fields = match.groupdict()\n",
    "    else:\n",
    "        fields = None\n",
    "\n",
    "    return fields\n",
    "\n",
    "\n",
    "def check_notes_without_matches(notes, decomposed_notes):\n",
    "    # number of notes that didnt match\n",
    "    n_no_match = len(notes) - len([note for note in decomposed_notes if note])\n",
    "\n",
    "    # index of each that didnt match\n",
    "    no_match_indexes = [idx for idx, note in enumerate(decomposed_notes) if not note]\n",
    "    # the notes themselves\n",
    "    no_match_notes = [notes[i] for i in no_match_indexes]\n",
    "\n",
    "    if no_match_indexes:\n",
    "        with ValueError as e:\n",
    "            e.add_note(f\"number of notes without match: {n_no_match}\")\n",
    "            e.add_note(f\"no match notes indexes: {no_match_indexes}\")\n",
    "            e.add_note(\n",
    "                f\"notes that didnt match:\\n\\n{pformat(dict(zip(no_match_indexes, no_match_notes)))}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def decompose_notes(notes) -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Decompose note strings in `notes` into a list of dicts of prespecified fields: 'datetime', 'title', 'content', 'tags'\n",
    "    \"\"\"\n",
    "    decomposed_notes = [extract_note_fields(note) for note in notes]\n",
    "    check_notes_without_matches(decomposed_notes=decomposed_notes, notes=notes)\n",
    "\n",
    "    if not any(decomposed_notes):\n",
    "        raise ValueError(\"a match was not found in a note\")\n",
    "\n",
    "    return decomposed_notes\n",
    "\n",
    "\n",
    "decomp_notes = decompose_notes(notes=notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'datetime': '2023-09-07T13:18:55',\n",
       "  'title': 'Adding nbstripout as precommit',\n",
       "  'content': 'Have added [nbstripout](https://github.com/kynan/nbstripout) as a pre-commit hook to ensure that notebooks are never commited with output. This hook will check if there is output, and clear it if so, simply requiring you to add the change before commiting again.',\n",
       "  'tags': '[wine_analysis_hplc_uv, notebooks, nbstripout, precommits, git, commit, log, project_management]'},\n",
       " {'datetime': '2024-05-03T15:24:29',\n",
       "  'title': 'Designing a Query API',\n",
       "  'content': \"I need specific logic for each table and column type. this is divided into: whether the input is an iterable or a scalar. What are the modes of operation? If no argument is submitted, dont add a WHERE to the query. if its an iterable, use IN, and if its a scalar, use '='. Further logic is needed for the wavelength and mins columns, where ease of use requires the input of ranges. I could continue to subset the relation as it is lazily evaluated, but in the interest of reducing the amount of python logic, we will instead use query string generation. Tterate through the options, and if present in the dict, initiate further logic UPDATE: this approach has been shelved, instead will assemble a full sampleset metadata table and filter on that, not large enough to need further optimization.\",\n",
       "  'tags': '[sql, query, tables, where, in, equals, wine_analysis_hplc_uv, log, project/etl/api]'},\n",
       " {'datetime': '2024-05-05T17:39:43',\n",
       "  'title': 'Establishing an ETL Orchastration Class',\n",
       "  'content': 'Have decided to establish an ETL orchastration class. It will be superficially modelled after the sklearn Pipeline, in that it will take a list of iniitalised transformation classes and execute them uing a method call \"execute_pipeline\". The Pipeline will expect the Transform classes to possess a \\'run\\' method which will be the top level class. On calling the Pipeline \"execute_pipeline\", the Pipeline object will iterate through the Transformer list and call their \"run\" methods. Each Transformer will also be responsible for providing a cleanup method to undo the action of \\'run\\', in the event of an error. The pipeline will execute each Transformer.run in the order they are provided, and if an error occurs, execute Transformer. Cleanup in reverse order.',\n",
       "  'tags': '[etl, sklearn, pipeline, wine_analysis_hplc_uv, log, project/etl/api]'},\n",
       " {'datetime': '2024-05-06T08:39:55',\n",
       "  'title': 'Pipeline and Transformers Completed',\n",
       "  'content': 'The pipeline and two tranformers: BuildSampleMetaData, CSWideToLong are completed and tested. Now to write a query class. Take a mapping of selection values and return a data table. Have an option to retrieve the chromatogram data or not. This query class will be found in \"queries.py\".',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, etl, pipeline, project/etl/api]'},\n",
       " {'datetime': '2024-05-06T10:00:00',\n",
       "  'title': 'Designing a Query Generator',\n",
       "  'content': \"Of course we could simply the whole thing by simply injecting strings from the filter dict... this indicates that we actually need two things - a query generator and a top level user API. The query generator will take a dict with keys pointing to the relevent field, iterate through it and generate the query string. The problem I am actually trying to solve is the slow time joining the metadata with the chromatogram images. We could reduce the complexity by creating the full metadata table as another table then simply filtering on that. Then we dont need to add the filter to the sub tables. Call it 'all_samples_metadata'\",\n",
       "  'tags': '[log, wine_analysis_hplc_uv, query, api, project/etl/api]'},\n",
       " {'datetime': '2024-05-07T20:01:43',\n",
       "  'title': 'Useful Regex',\n",
       "  'content': 'VSCodes \\'Find All References\\' function is untrustworthy. To find imports in the package, use this template \"<start of import>^([^\\\\s]+ ).*( .*)<end of import>\". See [stack overflow](https://stackoverflow.com/a/76129640/18650135)',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, regex, references]'},\n",
       " {'datetime': '2024-05-08T10:37:00',\n",
       "  'title': 'Recovering Old Deleted Files in Git',\n",
       "  'content': '[Stack Overflow](https://stackoverflow.com/a/44425132/18650135) has provided a one-liner to recover deleted files: ```sh git config --global alias.undelete \\'!sh -c \"git checkout $(git rev-list -n 1 HEAD -- $1)^ -- $1\" -\\'``` Use as  follows: ```sh git undelete path/to/file.ext``` And it will add the file back in, at its path. Note: does require a clean repo, or stashing any changes.',\n",
       "  'tags': '[git, file_recovery, file_management]'},\n",
       " {'datetime': '2024-05-09T16:08:13',\n",
       "  'title': 'SQL Table Comparisons',\n",
       "  'content': 'While verifying old tests, I decided to implement a simple comparison function. It should be straight foreward - get two tables, and do a series of comparisons to find similarities and differences, ala polars, pandas compare. But polars doesnt come with a comparison function, and calling for equality of `describe`, for example, where `describe` is used to summarize the functions is both discourged by the devs, and does not contain sufficient information about non numerical columns. Then I found that duckdb comes with a built-in SUMMARIZE method, but no table-to-table equality functions. So I envisioned a join and anti-join strategy, however it quickly became apparent that this was going to be a laborious task, to iterate through and compare each column, and to produce a similiarty/dissimilarity report. So i turned to third party options, and found data-diff. But data-diff was not constructed in the mind of comparing totally dissimilar tables, and furthermore the Python API is not well fleshed out or documented, and the return values are complex, requiring parsing to produce an easy-to-read report. All in all, no solution has worked yet. The main problem is that descriptions of similarity are arbitrary, as is expected behavior in the face of disimilarity. The main problem right now is that the generation of the stats dict requires id columns to be designated (?) I think the best course of action is to relegate `find_diff` to difference summaries and restrict my discussions of difference to booleans. Essentially, there are different levels of similarity, and different reports on the disimilarity for different levels.  1. If no columns in common, relegated to comparisons of geometry 2. if column names common, compare datatypes 3. if column names and data types the same, can compare. Two totally dissimilar tables cannot be compared at all.  There are set operations built in to duckdb as well. But how do I finish this in 15 minutes? polars `assert_frame_equal`. Thats how. Get the tables as frames and call it.',\n",
       "  'tags': '[wine_analysis_hplc_uv, log, project/etl/api, table, sql, comparison, polars, data-diff, python]'},\n",
       " {'datetime': '2024-05-10T10:07:07',\n",
       "  'title': 'Handling Database Connections',\n",
       "  'content': 'Modeling a programmatic flow while operating on a SQL database from a higher level language requires managing database connections. Connections are made by providing a configuration, a URL, or a filepath. In the case of duckdb, we use filepaths as it is a local-first database. In Pythons, database connections are modeled as objects, who can be passed like any other variable. This then raises the quetion of whether to a pass a live connection object, or the filepath string, and create a connection within every function. As duckdb has no problems creating multiple connection simultaneously for read and write operations (based on personal tests), then the decision becomes arbitrary. There is, however, one cosideration, which is that of performance. According to [this stack overflow thread](https://stackoverflow.com/a/65387376/18650135), creating connection objects within each scope is less perfromant than maintaining one object, and thus \"one connection per application (sic.)\" is best practice. TLDR: one connection object per application. Always write applications expecting a connection object, except for the user facing API.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, connections, sql, url, filepath]'},\n",
       " {'datetime': '2024-05-14T14:27:19',\n",
       "  'title': 'Refactoring \"notebooks\" Dir',\n",
       "  'content': \"a lot of development after June last year went into a catch-all 'notebooks' folder. This included preprocessing, dtw, xgboost, pca, etc. This obscured progress, (state of project advancement) behind somewhat arbritrary folder names and orphaned a lot of code as approaches changed throughout time. Furthermore code that should be scripts or modules is currently in notebooks. Further-furthermore, refactoring the code to keep it functioning and testable is currently not possible. Further-further-furthermore, notes and code are not clearly linked, and past refactoring has left hyperlinks without targets, where it appears that pylance does not update markdown hyperlinks in notebooks. The strategy to solve this dilemma is to pool everything together and seperate into modules based on  topic, with supporting notebooks. The deadline will be by EOD.\",\n",
       "  'tags': '[log, wine_analysis_hplc_uv, code, project_management, notebooks, dtw, xgboost, pca, project/refactoring_notebook_dir]'},\n",
       " {'datetime': '2024-05-14T14:41:21',\n",
       "  'title': 'Testing Notebooks',\n",
       "  'content': 'testing the notebooks will require that they function. This is difficult as in many cases I am lacking either input, output or intermediate data.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, project/refactoring_notebook_dir]'},\n",
       " {'datetime': '2024-05-15T08:49:40',\n",
       "  'title': 'From Python API to Super Table 2.0',\n",
       "  'content': 'I spent a lot of time yesturday developing a python api for the data retrieval query. A lot of time, mostly spent in input validation of a nested dict. Towards the end, while coming up with more complicated test cases, it occured to me to simply try joining the sample metadata and chromato-spectral tables together en masse, and see how long that takes. It took less than 20 seconds. Thus any UX improvement through the python API is lost on having to catch any possible edge-case. A better solution would be to simply produce the join table and query out of it.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, project/etl/api, sql, python, dict, tables]'},\n",
       " {'datetime': '2024-05-15T08:56:47',\n",
       "  'title': 'Vindication',\n",
       "  'content': 'Querying the large table takes time. For example returning the wine metadata for each row uniquely takes 11.5 seconds. The same query directly on the sample metadata table takes 400 micro seconds.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, project/etl/api, sql, python, dict, tables, queries, sample_metadata, speed]'},\n",
       " {'datetime': '2024-05-15T09:05:50',\n",
       "  'title': 'SQL Queries over Python',\n",
       "  'content': 'the python API is being dropped in favor of direct sql queries. I can write join several times, its fine.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, project/etl/api, sql, python, joins]'},\n",
       " {'datetime': '2024-05-15T09:30:45',\n",
       "  'title': 'Avoiding Column Duplication in DuckDB Joins with USING',\n",
       "  'content': 'when joining two tables on a column with the same name, use \"SELECT *\" in combination with \"USING\" instead of \"ON\" if you want to avoid duplicating the column.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, project/etl/api, sql, joins]'},\n",
       " {'datetime': '2024-05-15T11:06:51',\n",
       "  'title': 'Reviewing time_axis_characterisation_and_normalization',\n",
       "  'content': \"It makes it obvious that I really overcomplicate things. In the section 'Measuring Sampling Frequency' I mistake floating point error for real data and derive a method of finding the significant figure of differentation between time points, defining that as the maximum level of precision. Where its obvious from the calculated frequency that two decimal places is sufficient if time is converted from minutes to seconds. The data should in fact be kept in 'seconds'.\",\n",
       "  'tags': '[EDA, time_analysis, frequency, notebooks, log, project/post_build_library, project/etl/api]'},\n",
       " {'datetime': '2024-05-15T14:18:36',\n",
       "  'title': 'Have discovered that sample_metadata is missing 8 samples',\n",
       "  'content': 'Should have 175, only has 163. Furthermore, have discovered that the design of `build_sample_metadata`, makes it even harder to debug than the previous iteration..',\n",
       "  'tags': '[sql, sample_metadata, samples, log, project/post_build_library, project/etl/api]'},\n",
       " {'datetime': '2024-05-15T16:57:39',\n",
       "  'title': 'its all fucked',\n",
       "  'content': 'Nothing is working, everything is too rigid, too brittle.  I have rewritten the sample_metadata build query, but i cant test it because i cant parametrize the table name. Or cant i? I could write a python routine to replace the table name in the query with another and test on that.',\n",
       "  'tags': '[log, project/post_build_library, project/etl/api]'},\n",
       " {'datetime': '2024-05-16T06:36:18',\n",
       "  'title': \"Recycling 'build_sample_metadata'\",\n",
       "  'content': \"The function(?) has been gutted, all logic moved to the query, the transformer now simply reads and executes the query kept in 'create_sample_metadata.sql'.\",\n",
       "  'tags': '[log, project/post_build_library, project/etl/api,]'},\n",
       " {'datetime': '2024-05-16T10:51:25',\n",
       "  'title': 'Almost finished moving the post_build_library state to sql files',\n",
       "  'content': 'Current plan is to manage the state creation and testing through python, but simple executions of sql queries rather than logic on python side. Everything is in macros so that I can unit test and name the output other things such as \"tbl_test\" etc, as well as limit output to increase speed. Tests should check for correct number of rows, presence of nulls.',\n",
       "  'tags': '[build_library, sql, duckdb, log, project/post_build_library, project/etl/api]'},\n",
       " {'datetime': '2024-05-16T10:55:43',\n",
       "  'title': 'state management is hard, and I need to remember why Im doing this - it needs to be reproducible',\n",
       "  'content': 'And that means that all operations to get to the final state must be automated. Macros appear to be the best way of rendering each stage unit-testable. Unfortantely I cannot pass functions to other functions, nor tables as objects. Is what it is.',\n",
       "  'tags': '[log, project/post_build_library, project/etl/api, macros, sql, unit-tests]'},\n",
       " {'datetime': '2024-05-16T11:17:50',\n",
       "  'title': 'duckdb notes',\n",
       "  'content': '`show tables` shows the tables in the current schema. `show` or `show all tables` shows all tables in all schemas. See https://duckdb.org/docs/guides/meta/list_tables.html.',\n",
       "  'tags': '[duckdb, duckdb-notes,sql, show, tables, all]'},\n",
       " {'datetime': '2024-05-16T11:32:48',\n",
       "  'title': 'duckdb notes cont',\n",
       "  'content': 'Moving tables between schemas does not seem directly possible, rather a new table in the destination schema can be created as a copy of the original table, then dropping the original table, if so desired. Not much more verbose than a copy/rename command.',\n",
       "  'tags': '[duckdb, log, project/post_build_library, project/etl/api, sql]'},\n",
       " {'datetime': '2024-05-16T11:34:11',\n",
       "  'title': 'Managing sql development and state',\n",
       "  'content': \"Ok, i think i have a working flow. Macros are defined in a macro specific file. A number of macros are combined to produce a final macro, which is then called in a state management file that creates the final table. For example the state designated 'post build library', or 'pbl' is created from macros in 'post_build_library_macros.sql', and the state is created by the queries in 'post_build_library_state_create.sql'. This allows for modular development of the individual actions creating the state, allows them to be unit testable, and seperates the actions from the state creation.\",\n",
       "  'tags': '[duckdb,mres, managing-state, macros, log, project/post_build_library, project/etl/api]'},\n",
       " {'datetime': '2024-05-16T11:59:08',\n",
       "  'title': 'sql macro file ordering',\n",
       "  'content': 'Need to define an easy to read order. as the macros are fully functional, they are little more than wrapping each other like an onion. THus there is a time ordering to their existance, and their order in the file needs to reflect that for easy reading. From here I will order them in reverse time order, with the last one first.',\n",
       "  'tags': '[sql, development, functional-programming, log, project/post_build_library, project/etl/api]'},\n",
       " {'datetime': '2024-05-16T12:29:29',\n",
       "  'title': 'Testing the new state creation',\n",
       "  'content': \"The state creation flow mentioned earlier is verified, however I now need a method of testing it. There is sqllogictest, which is recommended by [duckdb](https://duckdb.org/docs/dev/sqllogictest/intro.html) but I cant find a Python intregration, and as my other tests are there I am loathe to change now. Python based unit tests calling the macros in the respective files and testing the output is probably the way to go, at least at this point. Simply put, and in the interest of saving time, just test the output of the 'create' macros. Just use the column count and estimated size from duckdb_tables\",\n",
       "  'tags': '[sql, mres, testing, log, project/post_build_library, project/etl/api]'},\n",
       " {'datetime': '2024-05-16T16:04:07',\n",
       "  'title': 'macro integration',\n",
       "  'content': 'it doesnt appear that macros are optimized, they seem to execute in entirety before moving on to the next statement. This means that they are not as useful as previously thought, because for example expensive queries in macros remain unoptimized. The search for testing options continues..',\n",
       "  'tags': '[sql, duckdb, macros, testing, log, macro, queries, sql, project/etl/api, project/post_build_library]'},\n",
       " {'datetime': '2024-05-16T23:17:35',\n",
       "  'title': 'Swapping macros for CTEs',\n",
       "  'content': \"So yeah, doesnt look like macros are the way foreward that I expected. I think we're gna have to do away with the idea of unit testing, and swap to CTEs, as they may be better optimized. Lets test it by recreating the code already written, but with ctes rather than macros. as you can reference ctes in other ctes, the functional nature of the code remains.\",\n",
       "  'tags': '[cte, log, code, macros, sql, project/etl/api, project/post_build_library]'},\n",
       " {'datetime': '2024-05-16T23:57:04',\n",
       "  'title': 'Observations from CTE implementation',\n",
       "  'content': 'Running the queries up until the melt - before the sample metadata join to add wine names - takes 47 seconds (not writing a table). The macro version runs for > 1 minute.',\n",
       "  'tags': '[sql, sql-macros, sql-cte, sql-unpivot, log, project/etl/api, project/post_build_library]'},\n",
       " {'datetime': '2024-05-17T09:24:31',\n",
       "  'title': 'Where to develop sql files',\n",
       "  'content': \"Am struggling to find a good flow. vscode has one extension, 'SQLTools', but it doesnt support the current release of DuckDB. DBeaver is ok but the ui is gross, the editing and keybinds are weird, and its handling of sql scripts is not in file? Harlequin was actually really good until i discovered that it doesnt have an autosave feature, and one crash undid 10 minutes of work. Neovim doesnt handle sql by default and i dont want to spend the time on that. And finally, redirecting sql files directly to duckdb results in poor formatting in the native terminal. One thing I hadnt considered, and considering I am all about TDD, was writing in sql files for execution in the python test file. we'll go with that today, so that we have an end.\",\n",
       "  'tags': '[log, project/etl/api, editors, TDD, sql, duckdb, neovim, harlequin-sql, python, dbeaver, terminal]'},\n",
       " {'datetime': '2024-05-17T10:32:12',\n",
       "  'title': 'finalizing development flow',\n",
       "  'content': \"TDD in python works. furthermore the cte approach with cs long seems much better than macro. Finally, now that we've played a bit more with cross language and platform development, its clear that I need to be creating packages as command line executable first.\",\n",
       "  'tags': '[log, chromatogram_spectra_long, macro, packages, cli, projceet/etl/api, project/post_build_library, tdd, python, sql, cte]'},\n",
       " {'datetime': '2024-05-17T10:34:21',\n",
       "  'title': 'whats next',\n",
       "  'content': 'I now need to finish testing the cs long query, and then write a script to get to the pbl state, from build library state. I also need to come up with better names for this stuff.',\n",
       "  'tags': '[tdd, mres, sql, python, build_library, project/etl/api, project/post_build_library, log, chromatogram_spectra_long]'},\n",
       " {'datetime': '2024-05-17T10:39:39',\n",
       "  'title': 'how to organise the state update',\n",
       "  'content': \"controling the flow of time from 'build library' to 'post build library' is difficult. The individual queries should be seperate to allow unit testing, for example 'sample_metadata' table creation shouldnt depend on 'chromatogram_spectra_long' creation, but how to organise it? This is where python comes in (or another scripting language). I can turn each of the calls into a python object, then call them sequentially, with tests if necessary.\",\n",
       "  'tags': '[python, automation, sql, build_library, project/etl/api, project/post_build_libray, log]'},\n",
       " {'datetime': '2024-05-17T11:49:40',\n",
       "  'title': 'modifying query strings for testing',\n",
       "  'content': 'Using simple string substitution to modify queries to make them test-friendly seems to work pretty well. For example modifying a tables name to point to a python run-time relation object. ',\n",
       "  'tags': '[log, project/post_build_library, project/etl/api, tdd, sql, duckdb, testing]'},\n",
       " {'datetime': '2024-05-17T11:50:43',\n",
       "  'title': 'bringing it all together',\n",
       "  'content': 'The state-setting queries are written and tested, but now I need to establish the flow. What does it need to do? 1. write chromatogram_spectra_long, 2. write sample_metadata, 3. join the wine and sample_num to chromatogram_spectra_long from sample_metadata. I believe we can create relation objects from a selection query in a .sql file then call the table creation with inline sql. Finally, we would need a simple outcome verification function - have these tables been created, do they match expectation. That module could then be called from the command line.',\n",
       "  'tags': '[log, chromatogram_spectra_long, sample_metadata, tdd, build_library, sql, duckdb, mres, python, project/etl/api, project/post_build_library]'},\n",
       " {'datetime': '2024-05-17T13:57:25',\n",
       "  'title': 'where ctes go in the query',\n",
       "  'content': \"ctes are part of the select statement, and as such for actions such as 'create table x as ..', the 'create table as..' component goes first, THEN the cte declaration, then the selection.\",\n",
       "  'tags': '[syntax, sql, cte, sql, create, table]'},\n",
       " {'datetime': '2024-05-17T14:30:31',\n",
       "  'title': 'updated test flow',\n",
       "  'content': 'Use transactions, drop tables / columns, etc, call the script to make the  change, observe if the change is as expected. then we dont have to worry about modifying the script.',\n",
       "  'tags': '[tdd, sql, mres, sql-transactions, sql, transaction, drop, table, column, script]'},\n",
       " {'datetime': '2024-05-17T14:31:42',\n",
       "  'title': 'friday roundup',\n",
       "  'content': \"im out of time, i can do a little bit more after work, but really i wont be able get back to this till monday. WE need to finish testing / devving the pbl script, but we've got the actions now, and we can unit test easily. once thats done, finish cleaning up the notebooks - do that by summarizing the HELL out of them, they are way too verbose to be left as they are. for example the time offset analysis is a 1 liner. doesnt need all of the other dross. have the query to express the distribution as well, but thats it. Reduce each to a paragraph and associated query.\",\n",
       "  'tags': '[log, mres, friday-roundup, notebooks, preprocessing, tdd, time_offset, project/etl/api, project/post_build_library, notebooks]'},\n",
       " {'datetime': '2024-05-17T15:44:57',\n",
       "  'title': 'perfecting the test database',\n",
       "  'content': \"While finishing the testing for the pbl state creation function, I have run into an unexpected stall while trying to add the wine and sample_num columns to chromatogram_spectrum_long. It is not clear why this is happening, and the development cycle is too long. I dont need to test on every single sample, as queries are taking too long to run. As it currently stands, there are about a million rows per sample. reducing the test set to 10 samples will still include 10 million rows. A method of filtering would be to join on sample_metadata after taking a sample from it. TODO: create the sample set, get a list of 10 samples so that you have deterministic outcome, create the queries in 'create_test_schema.sql'.\",\n",
       "  'tags': '[log, project/etl/api, project/post_build_library, sql, chromatogram_spectra_long, wine, sample_num, column, todo]'},\n",
       " {'datetime': '2024-05-17T23:51:36',\n",
       "  'title': 'a step back',\n",
       "  'content': \"I dont think I checked the query sequence with a simple limit added. Try that before introducing another table. Also, try the table replacement vs. the update, it might be that the update is more taxing. For the second point - it takes 12 seconds to execute the 'alter table' query, and 24 for the 'as join' query. Altering the table as opposed to completely replacing it takes half the time. The motivation for creating the tes ttable is that there is no select in the query so I cannot limit(?) the query size. But i can limit the size of chromatogram_spectra_long in its creation query. that is the easiest way. Or you could redefine the object structure to output a list of queries, then use execute many rather than iterating.\",\n",
       "  'tags': '[sql, log, project/etl/api, project/post_build_library, sql, query, update, join]'},\n",
       " {'datetime': '2024-05-18T00:13:11',\n",
       "  'title': 'confirming earlier results',\n",
       "  'content': 'confirmed, something odd is happening with the alter table query. However when limiting the table to 10 rows it functions as expected. 1,000,000 rows works as expected, 13 sec execution time. At 10 million rows the slow query behavior reappears, but finishes after 2 minutes 15 seconds. What happens if we use the join approach instead? It takes 1 minute and 15 seconds on 10 million rows, and.. 4 minutes to complete the query on the total dataset. The other option to test is whether its faster on the wide vesrion of the table, and whether it slows down the unpivot. so, todos: 1. add index and try. 2. add names and sample_num before unpivot.',\n",
       "  'tags': '[log, project/etl/api, project/post_build_library, alter, table, sql, unpivot, index]'},\n",
       " {'datetime': '2024-05-18T00:35:07',\n",
       "  'title': 'benefits of indexes',\n",
       "  'content': 'Apparently indexes can help speed up operations such as joins. Should add some to the tables and see if anything changes. Need to add an index to chromatogram_spectra_long and sample_metadata, both being id.',\n",
       "  'tags': '[sql, index, joins, mres, project/post_build_library, log, project/etl/api, build_library, chromatogram_spectra_long, sample_metadata]'},\n",
       " {'datetime': '2024-05-18T01:16:07',\n",
       "  'title': 'results of adding the index',\n",
       "  'content': 'Nothing changed for the join. and the alter table? I cancelled it after 7 minutes.',\n",
       "  'tags': '[sql, index, duckdb, mres, join, alter, table, log, project/etl/api, project/post_build_library]'},\n",
       " {'datetime': '2024-05-18T02:29:21',\n",
       "  'title': 'A Result',\n",
       "  'content': \"We're now encountering OOM errors while executing the queries in series.\",\n",
       "  'tags': '[project/etl/api, project/post_build_library, duckdb, query, python]'},\n",
       " {'datetime': '2024-05-18T10:31:58',\n",
       "  'title': 'todo',\n",
       "  'content': 'fiinish fixing build_library_oop tests, commit them and everything else in pbl.',\n",
       "  'tags': '[log, project/etl/api, project/build_library/post_build_library]'},\n",
       " {'datetime': '2024-05-19T00:04:57',\n",
       "  'title': 'a way forward',\n",
       "  'content': 'A eureka moment. Data analysis design and develpment should be functional first. we are for the most part interested in going from state A to state B through a series of transformations, usually of a 1 dimensional array. The properties of the current state are not actually interesting, only the final state, therefore a functional approach removes a lot of overhead. Therefore, a TDD appraoch coupled with functional-first, possibly with a OOP API is how I will proceed. This makes testing easy, as state is very easy to manage, and allows us to wrap the top level functions in whatever structure we need for interfacing with other libraries, such as sci-kit learn transformers. Thus, a final paradigm has emerged. top level transformer functions first, class based apis if necessary, with intermediate state managed by polars pipes. Also, as much duckdb as possible. Or something like that.',\n",
       "  'tags': '[log, project/deconvolution, tdd, testing, oop, sklearn, programming, polars, duckdb]'},\n",
       " {'datetime': '2024-05-20T08:46:40',\n",
       "  'title': 'Finishing deconvolution and integration',\n",
       "  'content': \"Time to bring it all together. To do so, I need to migrate my code from hplc-py to wine_analysis_hplc_uv. Then I need to rebuild the pipe and test it with a sample dataset, then with real data. To simplify things, I want to discard scikit learn for now, and focus on honing the pipeline steps as pure functions, then wrap the pure functions in the scikit learn api downtrack. So by 2pm we'll have some results. Therefore, timeframe: 9 - 10: complete migration, set up environment. 10 - 12: run sampleset, debug. 12 - 1: running real dataset. 1 - 2: clean up. It seems an unrealistically rapid pace, but lets see how we go. Specific notes will be at [Deconvolution Migration](../README.md#deconvolution-migration).\",\n",
       "  'tags': '[package-management, deconvolution,signal-processing, sklearn, deconvolution, project/hplc_py_migration, scikit, ]'},\n",
       " {'datetime': '2024-05-21T21:44:16',\n",
       "  'title': 'back on track',\n",
       "  'content': \"Ok, package seems to be healthy again? and we're now running 3.12. I guess we'll have to run tests to find out.\",\n",
       "  'tags': '[log, project/migrating_to_python_312]'},\n",
       " {'datetime': '2024-05-28T23:24:39',\n",
       "  'title': 'is normal distribution scale equal to peak width? As per title',\n",
       "  'content': 'Im not sure about this. Cant find any informatino on the net, so best to just test it. Easiest way will be to generate a pdf then measure it and compare. The answer? related but not equal. The literature is in agreement. The scale is a parameter that describes the spread of the distribution across x, but as the AUC is always 1, increasing scale decreases height.',\n",
       "  'tags': '[normal_distribution, log, project/convoluted_skewnorm_generation, auc, scale, width]'},\n",
       " {'datetime': '2024-05-28T23:36:19',\n",
       "  'title': 'observations of the parameters of the normal distribution',\n",
       "  'content': \"A 'normal' looking distribution has a scale of 10% of the range of x, and location half the range of x. Increasing the scale reduces the y maxima, and vice versa, but it appears that the area is always the same, equal to 1 (before transformation). Hence height and scale are always working in inverse. According to [statisticshowto](https://www.statisticshowto.com/scale-parameter/), in the standard normal distribution, scale is equal to standard deviation, or half the peak width at a height somewhere vaguely above the half height point, depending on the scale. As we recall, the standard deviation is equal to the square root of the  sum of the mean residuals and observations divided by the number of observations. Thus it is clear that the use of peak width measured at a relative height of 1 is not attempting to estimate distribution scale at all, but it is simply an easy means of providing an initial guess. One which I would say is flawed, but enough to produce results. Finding a better means of estimating scale may be a route to increasing the performance.\",\n",
       "  'tags': '[log, normal_distribution, skew_norm, deconvolution, peak_width, peak_height, peak_mapping ,mres, scale, height, residuals, project/deconvolution, area, project/deconvolution, project/convoluted_skewnorm_generation]'},\n",
       " {'datetime': '2024-05-29T09:54:56',\n",
       "  'title': 'Role of Scale in the Skew-Normal Distribution',\n",
       "  'content': 'But what is the role of scale in the skew-normal distribution? [wikipedia](https://en.wikipedia.org/wiki/Skew_normal_distribution) it appears that the skew-normal only introduces the skew as a coefficient to the x term, thus the scale role should be the same, modified by skew. So we can assume that half the width at half height is a good approximation of the scale of the peak. Ergo, that measurement should allow an approximation of the peak, decreasing in efficacy as the skew increases.',\n",
       "  'tags': '[log, skew_norm, scale, width, height, peak, project/deconvolution, project/convoluted_skewnorm_generation]'},\n",
       " {'datetime': '2024-05-29T10:01:49',\n",
       "  'title': 'Observing the Effect of Skew on Peak Width Measurements Through Topographic Methods',\n",
       "  'content': 'I should answer the following questions - how does measured peak width change as skew increases? How does model fit change as skew increases? And what about changing the width measurement height, what effects does that have?',\n",
       "  'tags': '[log, project/deconvolution, project/convoluted_skewnorm_generation, skew, width, topography, model, height]'},\n",
       " {'datetime': '2024-05-29T10:07:36',\n",
       "  'title': 'How does Measured Peak Width Change with Skew? To answer this, we would need a function to generate the pdf, then measure the peak width of the pdf',\n",
       "  'content': \"The underlying factor is this - measured peak width is dependent on peak prominence, which works for isolated peaks but suffers as peaks become convoluted, because the 'center' of the peak as measured becomes higher and higher (need to confirm this).\",\n",
       "  'tags': '[peak_width, project/deconvolution, project/convoluted_skewnorm_generation, peak, prominence, log]'},\n",
       " {'datetime': '2024-05-29T09:39:30',\n",
       "  'title': 'musing on estimation of scale through peak width',\n",
       "  'content': 'Cremerlab does not emphasise how precarious that assumption is, as the more convoluted the peak, the less its prominence, and thus the relative height has more weight when compared to an unconvoluted signal. Is that true? Again, I need a means of generating signals in order to explore this. Perhaps it would be beter to use sinosoidal waves.',\n",
       "  'tags': '[project/deconvolution, cremerlab, peak_scale, peak_width, prominance, relative_height, weight, deconvolution, signals, sinosoidal_waves, log, project/convoluted_skewnorm_generation,]'},\n",
       " {'datetime': '2024-05-29T15:59:13',\n",
       "  'title': 'absurd day',\n",
       "  'content': \"So I tried to write a short piece on the skew normal distribution, as per the notes above. But where to put the notes? I decided to structure the notes directory as per the tests, mirroring the package. Fine and dandy, but to then enact that structure I had to clean up the notes already present, also fine, but they needed cleaning up a little bit first.. right? and while im going, I might as well delete any unnecessary ones. Oh and why am I in both ipynb and qmd files? Best convert everything to qmd on the way.. wait how do I structure them now, as when rendering the qmd files the directories get cluttered with the html files.. Oh and these dont actually run so I need to fix them to make sure that they have everything that.. wait the production database is not up to date? Ok better run the post build-library pipeline.. wait theres no docs, a bunch of todos, and no clear method of running it? Fucking excellent. But at least the tests were completed, we can execute those as we refactor then see if itll work in prod. OH WAIT, PYTEST CANT COLLECT THE TESTS BECAUSE OF ERORS THROUGHOUT OTHER TESTS!! So now we're fixing random tests so we can refactor the pbl directory to provide a UI. So i can get the prod db up to date so I can finish refactoring documents from 9 months ago so I can finish organising my notes so I can write new notes about the norm distribution so I can finish rewriting the deconvolution module so I can deconvolute my signals so I can bin the peaks so I can compare across the samples so I can write a simple EDA report. New rule - commit anything by 5, on a seperate branch if need be. In fact, it should always be on a seperate branch, manage your dependencies better, no classes, and never do anything like this ever again.\",\n",
       "  'tags': '[project/deconvolution, skew_normal_distribution, roundup, project_management, quarto, tests, pbl, python, log, project/convoluted_skewnorm_generation]'},\n",
       " {'datetime': '2024-05-29T16:34:12',\n",
       "  'title': 'post_build_library in production',\n",
       "  'content': 'Have for the first time successfully execute `pbl_state_creation.update_library` on the production database. A milestone for sure. Have not written it as a standalone script, as its a simple matter to create the connection object and call the function on it.',\n",
       "  'tags': '[project/etl/api, project/post_build_library, mres, duckdb, log]'},\n",
       " {'datetime': '2024-05-30T11:29:12',\n",
       "  'title': 'notes on killing processes',\n",
       "  'content': 'Quarto is very much still in beta, and I am running into a problem where if it fails to render a document half way through, and has a currently open duckdb connection, it is not closing the connection, holding the lock in place. While I could context management to handle the connections, that kills a sense of flow, the whole point of the document. So I needed a means of killing the running process without restarting the computer. In comes bash `kill`, which as it sounds, kills running processes according to their PID, usefully provided by duckdb on `IOException` error.',\n",
       "  'tags': '[log, project/note_cleanup,bash ,duckdb, quarto, error, shell, process, PID, kill]'},\n",
       " {'datetime': '2024-05-30T12:29:50',\n",
       "  'title': 'rewriting dtw notes',\n",
       "  'content': 'The dtw notes are a cesspit of over-abstracted functions, missing data files, pandas multiindex madness and missing documentation. Its going to take days to fix, and ive got hours. even less than that. Tecnically ive got an hour and a half to finish the thesis. lol. it can remain as it is, and will be left to the very end.',\n",
       "  'tags': '[project/note_cleanup, dtw, log, thesis,pandas]'},\n",
       " {'datetime': '2024-05-30T12:35:30',\n",
       "  'title': 'Continuing on note organisation',\n",
       "  'content': 'The rewriting and organising of existing notes is going well, the mirroring of the package structure in the notes directory is promising, and while the conversion to qmd has not been without hiccups, overall i prefer the format to ipynb, plain text is always preferable. One problem is code formatting, but it is a very late stage concern.',\n",
       "  'tags': '[log, project/note_cleanup, project_structure, quarto, ipython_notebooks, code, formatting, mres]'},\n",
       " {'datetime': '2024-05-31T03:51:10',\n",
       "  'title': 'operations on long tables, or 175x10^6 rows is too much',\n",
       "  'content': \"As title. Cant do shit, and its wasting all my time. My previous approach was to apply the transformations on selection, adding layers and layers in order to reach the current *state*. it's bullshit though. There has to be an approach that works. What I need is a community that can point me in the right direction. Also how do we make the query work? we first figure out what arithmatic operations DO work, and whether we need to create a temporary table in between operations. Or. Another table. Create a cs table at 256 nm for further operations. At this point in time the deconvolution precludes a 3d dataset, at least until the operation can be expanded to 3 dimensions.\",\n",
       "  'tags': '[log, project/etl/api, sql, chromatogram_spectra, deconvolution, data_3d]'},\n",
       " {'datetime': '2024-06-04T16:54:36',\n",
       "  'title': 'tuesday roundup',\n",
       "  'content': \"I've been busy. When I last touched base I was trying to rewrite the time offset correction as a dataset wide arithmatic query that was refusing to complete within an acceptable time. Since then I realised that I was expecting too much of the database and my computer, and have since decided to do the following: TODO: denormalise the time column into a table of mins with sample id and row idx as key. Replace with the row idx in 'cs_long'. Then time based operations can be completed on the 1 time column that is the same for evey wavelength in the sample, rather than trying to compute the same thing for every wavelength in every sample, reducing the size of the operation by the number of wavelengths in each sample, approximately (600-192)/2 * 175 = 35,700, or reducing it by a factor of 6, which will certainly be a managable operation. In the meantime I've been cleaning up my notes and old modules with the intention of bringing those spaces back to life, i.e. have a core signal processing module that contains all the methods required, and get all my notes running again so I can use them to publish. Finally, aggregating all my notes together so that topics emerge, from the topics, articles, chapters etc. Ergo, the package structure needs to be perfectly reflected in the tests structure, and the notes structure, and also the zotero collection structure. Thus the content exists across these four platforms, and they will in essence keep each other in check. Need a note for the function, tests for the function, references for the note. the only thing im missing now is to A: figure out a better view for zotero collections, as you are able to store items within a collection, but subcollections are treated differently - want a file-explorer type display, subcollections and items treated as objects of their containing collection, and b. tests for notebooks to ensure that modifications dont break them.\",\n",
       "  'tags': '[log, roundup, offest, sql, database, denormalization, project/time_offset_correction, project/time_denormalization, project/note_cleanup, zotero, collections, project/zotero_cleanup]'},\n",
       " {'datetime': '2024-06-05T11:04:14',\n",
       "  'title': 'Note taking',\n",
       "  'content': 'Again trying come up with a frictionless note taking process. Atm I am forming an outline from a source and restructuring it as a structure becomes clear. Furthermore, in lieu of being able to form inline links, creating seperate qmd documents and then cross-referencing them seems to be the way to go, but like everything else in quarto its.. janky. I have moved to using author name, short title and year as cite key to give more context, btw.',\n",
       "  'tags': '[quarto, thesis, note-taking, qmd]'},\n",
       " {'datetime': '2024-06-05T11:05:39',\n",
       "  'title': 'quarto cross referencing and yaml title field',\n",
       "  'content': 'Cant cross reference the yaml title if using that, and the numbering includes the title and then the scond title if you use that instead, so if u want to cross reference, dont use the yaml field.',\n",
       "  'tags': '[cross_referencing, yaml, title, quarto]'},\n",
       " {'datetime': '2024-06-05T13:26:00',\n",
       "  'title': 'quarto reference figures and code cells',\n",
       "  'content': 'code cell output is treated like figures, who have a comment pipe syntax for their relationship with the document and referencing, depending on the syntax of the code in question. For python it is \"#|\", for mermaid \"%%|\". Quarto scans the code for the comment pipes then acts on the command. For alignment, use `#|layout-align`, for cross-referencing, first label with `#|label: \\'a-label\\'`. See [diagrams](https://quarto.org/docs/authoring/diagrams.html), [cross-references](https://quarto.org/docs/authoring/cross-references.html), [panel-layout](https://quarto.org/docs/reference/cells/cells-jupyter.html#panel-layout), [diagrams, cross-references](https://quarto.org/docs/authoring/diagrams.html#cross-references). Captions are added by specifying `fig-cap`, with a heirarchical `sub-cap` option [subcaptions](https://quarto.rocks/authoring/figures#subcaptions), [figures, subcaptions](https://quarto.org/docs/authoring/figures.html#subcaptions). Note that captions are left aligned by default with no option to change. There are css hacks that promise to modify this, but I have not managed to make it work [alignment of figure captions](https://github.com/quarto-dev/quarto-cli/discussions/7003#discussioncomment-7112704), [center-caption-in-quarto](https://stackoverflow.com/questions/76404758/center-caption-in-quarto).',\n",
       "  'tags': '[quarto, references, figures, code_cells, mermaid, comments, layout, cross-referencing, captions, css]'},\n",
       " {'datetime': '2024-06-07T08:55:32',\n",
       "  'title': 'recycling hplc-py',\n",
       "  'content': 'TODO: digest the resolution enhancement notebooks. Then digest the methods.',\n",
       "  'tags': '[hplc-py, project/deconvolution, notebooks]'},\n",
       " {'datetime': '2024-06-07T08:57:17',\n",
       "  'title': 'another attempt at proof of concept',\n",
       "  'content': \"Need to develop a rudimentary deconv pipeline THEN fine tune it. As I've experienced for literally the last 6 months though, its not easy, there are too many moving parts. The last attempt (in the hplc-py fork) failed because I tried to be too fancy, and the individual components were not encapsulated enough, which raised the complexity to unmangable levels. A clean, pure function and module based approach is in my opinion the best approach for development, then wrapping everything in a class IF NECESSARY.\",\n",
       "  'tags': '[project/deconvolution, encapsulation, functions, class]'},\n",
       " {'datetime': '2024-06-07T09:01:59',\n",
       "  'title': 'moving forward',\n",
       "  'content': 'Researching dsp is my best bet at finishing this. But the code contains the results, and the structure, they need to grow in tandem. The notes and the code. The notes, the code and the tests. However, my eyes are starting to give me trouble, as they do every time I start any serious reading. Working on the notes and code alternatively seems to be a good approach, as my eyes dont suffer as much when coding.',\n",
       "  'tags': '[project/deconvolution, digital_signal_processing, research, eyes, coding]'},\n",
       " {'datetime': '2024-06-12T11:49:09',\n",
       "  'title': \"Merging 'moving_notes_writing_deconv' With Main\",\n",
       "  'content': \"As title, the deconvolution project has been shelved whiel I finish chapter 1. in the interest of avoiding muddying the history, i'll merge to main now.\",\n",
       "  'tags': '[project/deconvolution, chapter_1, project_management]'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate datetimes\n",
    "\n",
    "from dateutil.parser import parse, ParserError\n",
    "\n",
    "\n",
    "def validate_datetimes(notes: list[dict[str, str]]):\n",
    "    no_parse_date_notes = []\n",
    "\n",
    "    for note in notes:\n",
    "        old_dt = note[\"datetime\"]\n",
    "        try:\n",
    "            new_dt = parse(old_dt).isoformat()\n",
    "        except ParserError:\n",
    "            new_dt = old_dt\n",
    "            no_parse_date_notes.append(note)\n",
    "        note[\"datetime\"] = new_dt\n",
    "\n",
    "    if no_parse_date_notes:\n",
    "        n_no_parse = len(no_parse_date_notes)\n",
    "        date_titles = [\n",
    "            {k: v for k, v in note.items() if k in [\"title\", \"datetime\"]}\n",
    "            for note in no_parse_date_notes\n",
    "        ]\n",
    "        err_str = f\"Some note datetimes were unable to be parsed. {n_no_parse} were not parsed. They are as follows:\\n\\n{pformat(date_titles)}\"\n",
    "        raise ValueError(err_str)\n",
    "    return notes\n",
    "\n",
    "\n",
    "validate_datetimes(notes=decomp_notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'datetime': '2023-09-07T13:18:55',\n",
       "  'title': 'Adding nbstripout as precommit',\n",
       "  'content': 'Have added [nbstripout](https://github.com/kynan/nbstripout) as a pre-commit hook to ensure that notebooks are never commited with output. This hook will check if there is output, and clear it if so, simply requiring you to add the change before commiting again.',\n",
       "  'tags': '[wine_analysis_hplc_uv, notebooks, nbstripout, precommits, git, commit, log, project_management]',\n",
       "  'filename': 'adding_nbstripout_as_precommit.md'},\n",
       " {'datetime': '2024-05-03T15:24:29',\n",
       "  'title': 'Designing a Query API',\n",
       "  'content': \"I need specific logic for each table and column type. this is divided into: whether the input is an iterable or a scalar. What are the modes of operation? If no argument is submitted, dont add a WHERE to the query. if its an iterable, use IN, and if its a scalar, use '='. Further logic is needed for the wavelength and mins columns, where ease of use requires the input of ranges. I could continue to subset the relation as it is lazily evaluated, but in the interest of reducing the amount of python logic, we will instead use query string generation. Tterate through the options, and if present in the dict, initiate further logic UPDATE: this approach has been shelved, instead will assemble a full sampleset metadata table and filter on that, not large enough to need further optimization.\",\n",
       "  'tags': '[sql, query, tables, where, in, equals, wine_analysis_hplc_uv, log, project/etl/api]',\n",
       "  'filename': 'designing_a_query_api.md'},\n",
       " {'datetime': '2024-05-05T17:39:43',\n",
       "  'title': 'Establishing an ETL Orchastration Class',\n",
       "  'content': 'Have decided to establish an ETL orchastration class. It will be superficially modelled after the sklearn Pipeline, in that it will take a list of iniitalised transformation classes and execute them uing a method call \"execute_pipeline\". The Pipeline will expect the Transform classes to possess a \\'run\\' method which will be the top level class. On calling the Pipeline \"execute_pipeline\", the Pipeline object will iterate through the Transformer list and call their \"run\" methods. Each Transformer will also be responsible for providing a cleanup method to undo the action of \\'run\\', in the event of an error. The pipeline will execute each Transformer.run in the order they are provided, and if an error occurs, execute Transformer. Cleanup in reverse order.',\n",
       "  'tags': '[etl, sklearn, pipeline, wine_analysis_hplc_uv, log, project/etl/api]',\n",
       "  'filename': 'establishing_an_etl_orchastration_class.md'},\n",
       " {'datetime': '2024-05-06T08:39:55',\n",
       "  'title': 'Pipeline and Transformers Completed',\n",
       "  'content': 'The pipeline and two tranformers: BuildSampleMetaData, CSWideToLong are completed and tested. Now to write a query class. Take a mapping of selection values and return a data table. Have an option to retrieve the chromatogram data or not. This query class will be found in \"queries.py\".',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, etl, pipeline, project/etl/api]',\n",
       "  'filename': 'pipeline_and_transformers_completed.md'},\n",
       " {'datetime': '2024-05-06T10:00:00',\n",
       "  'title': 'Designing a Query Generator',\n",
       "  'content': \"Of course we could simply the whole thing by simply injecting strings from the filter dict... this indicates that we actually need two things - a query generator and a top level user API. The query generator will take a dict with keys pointing to the relevent field, iterate through it and generate the query string. The problem I am actually trying to solve is the slow time joining the metadata with the chromatogram images. We could reduce the complexity by creating the full metadata table as another table then simply filtering on that. Then we dont need to add the filter to the sub tables. Call it 'all_samples_metadata'\",\n",
       "  'tags': '[log, wine_analysis_hplc_uv, query, api, project/etl/api]',\n",
       "  'filename': 'designing_a_query_generator.md'},\n",
       " {'datetime': '2024-05-07T20:01:43',\n",
       "  'title': 'Useful Regex',\n",
       "  'content': 'VSCodes \\'Find All References\\' function is untrustworthy. To find imports in the package, use this template \"<start of import>^([^\\\\s]+ ).*( .*)<end of import>\". See [stack overflow](https://stackoverflow.com/a/76129640/18650135)',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, regex, references]',\n",
       "  'filename': 'useful_regex.md'},\n",
       " {'datetime': '2024-05-08T10:37:00',\n",
       "  'title': 'Recovering Old Deleted Files in Git',\n",
       "  'content': '[Stack Overflow](https://stackoverflow.com/a/44425132/18650135) has provided a one-liner to recover deleted files: ```sh git config --global alias.undelete \\'!sh -c \"git checkout $(git rev-list -n 1 HEAD -- $1)^ -- $1\" -\\'``` Use as  follows: ```sh git undelete path/to/file.ext``` And it will add the file back in, at its path. Note: does require a clean repo, or stashing any changes.',\n",
       "  'tags': '[git, file_recovery, file_management]',\n",
       "  'filename': 'recovering_old_deleted_files_in_git.md'},\n",
       " {'datetime': '2024-05-09T16:08:13',\n",
       "  'title': 'SQL Table Comparisons',\n",
       "  'content': 'While verifying old tests, I decided to implement a simple comparison function. It should be straight foreward - get two tables, and do a series of comparisons to find similarities and differences, ala polars, pandas compare. But polars doesnt come with a comparison function, and calling for equality of `describe`, for example, where `describe` is used to summarize the functions is both discourged by the devs, and does not contain sufficient information about non numerical columns. Then I found that duckdb comes with a built-in SUMMARIZE method, but no table-to-table equality functions. So I envisioned a join and anti-join strategy, however it quickly became apparent that this was going to be a laborious task, to iterate through and compare each column, and to produce a similiarty/dissimilarity report. So i turned to third party options, and found data-diff. But data-diff was not constructed in the mind of comparing totally dissimilar tables, and furthermore the Python API is not well fleshed out or documented, and the return values are complex, requiring parsing to produce an easy-to-read report. All in all, no solution has worked yet. The main problem is that descriptions of similarity are arbitrary, as is expected behavior in the face of disimilarity. The main problem right now is that the generation of the stats dict requires id columns to be designated (?) I think the best course of action is to relegate `find_diff` to difference summaries and restrict my discussions of difference to booleans. Essentially, there are different levels of similarity, and different reports on the disimilarity for different levels.  1. If no columns in common, relegated to comparisons of geometry 2. if column names common, compare datatypes 3. if column names and data types the same, can compare. Two totally dissimilar tables cannot be compared at all.  There are set operations built in to duckdb as well. But how do I finish this in 15 minutes? polars `assert_frame_equal`. Thats how. Get the tables as frames and call it.',\n",
       "  'tags': '[wine_analysis_hplc_uv, log, project/etl/api, table, sql, comparison, polars, data-diff, python]',\n",
       "  'filename': 'sql_table_comparisons.md'},\n",
       " {'datetime': '2024-05-10T10:07:07',\n",
       "  'title': 'Handling Database Connections',\n",
       "  'content': 'Modeling a programmatic flow while operating on a SQL database from a higher level language requires managing database connections. Connections are made by providing a configuration, a URL, or a filepath. In the case of duckdb, we use filepaths as it is a local-first database. In Pythons, database connections are modeled as objects, who can be passed like any other variable. This then raises the quetion of whether to a pass a live connection object, or the filepath string, and create a connection within every function. As duckdb has no problems creating multiple connection simultaneously for read and write operations (based on personal tests), then the decision becomes arbitrary. There is, however, one cosideration, which is that of performance. According to [this stack overflow thread](https://stackoverflow.com/a/65387376/18650135), creating connection objects within each scope is less perfromant than maintaining one object, and thus \"one connection per application (sic.)\" is best practice. TLDR: one connection object per application. Always write applications expecting a connection object, except for the user facing API.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, connections, sql, url, filepath]',\n",
       "  'filename': 'handling_database_connections.md'},\n",
       " {'datetime': '2024-05-14T14:27:19',\n",
       "  'title': 'Refactoring \"notebooks\" Dir',\n",
       "  'content': \"a lot of development after June last year went into a catch-all 'notebooks' folder. This included preprocessing, dtw, xgboost, pca, etc. This obscured progress, (state of project advancement) behind somewhat arbritrary folder names and orphaned a lot of code as approaches changed throughout time. Furthermore code that should be scripts or modules is currently in notebooks. Further-furthermore, refactoring the code to keep it functioning and testable is currently not possible. Further-further-furthermore, notes and code are not clearly linked, and past refactoring has left hyperlinks without targets, where it appears that pylance does not update markdown hyperlinks in notebooks. The strategy to solve this dilemma is to pool everything together and seperate into modules based on  topic, with supporting notebooks. The deadline will be by EOD.\",\n",
       "  'tags': '[log, wine_analysis_hplc_uv, code, project_management, notebooks, dtw, xgboost, pca, project/refactoring_notebook_dir]',\n",
       "  'filename': 'refactoring_\"notebooks\"_dir.md'},\n",
       " {'datetime': '2024-05-14T14:41:21',\n",
       "  'title': 'Testing Notebooks',\n",
       "  'content': 'testing the notebooks will require that they function. This is difficult as in many cases I am lacking either input, output or intermediate data.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, project/refactoring_notebook_dir]',\n",
       "  'filename': 'testing_notebooks.md'},\n",
       " {'datetime': '2024-05-15T08:49:40',\n",
       "  'title': 'From Python API to Super Table 2.0',\n",
       "  'content': 'I spent a lot of time yesturday developing a python api for the data retrieval query. A lot of time, mostly spent in input validation of a nested dict. Towards the end, while coming up with more complicated test cases, it occured to me to simply try joining the sample metadata and chromato-spectral tables together en masse, and see how long that takes. It took less than 20 seconds. Thus any UX improvement through the python API is lost on having to catch any possible edge-case. A better solution would be to simply produce the join table and query out of it.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, project/etl/api, sql, python, dict, tables]',\n",
       "  'filename': 'from_python_api_to_super_table_2.0.md'},\n",
       " {'datetime': '2024-05-15T08:56:47',\n",
       "  'title': 'Vindication',\n",
       "  'content': 'Querying the large table takes time. For example returning the wine metadata for each row uniquely takes 11.5 seconds. The same query directly on the sample metadata table takes 400 micro seconds.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, project/etl/api, sql, python, dict, tables, queries, sample_metadata, speed]',\n",
       "  'filename': 'vindication.md'},\n",
       " {'datetime': '2024-05-15T09:05:50',\n",
       "  'title': 'SQL Queries over Python',\n",
       "  'content': 'the python API is being dropped in favor of direct sql queries. I can write join several times, its fine.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, project/etl/api, sql, python, joins]',\n",
       "  'filename': 'sql_queries_over_python.md'},\n",
       " {'datetime': '2024-05-15T09:30:45',\n",
       "  'title': 'Avoiding Column Duplication in DuckDB Joins with USING',\n",
       "  'content': 'when joining two tables on a column with the same name, use \"SELECT *\" in combination with \"USING\" instead of \"ON\" if you want to avoid duplicating the column.',\n",
       "  'tags': '[log, wine_analysis_hplc_uv, project/etl/api, sql, joins]',\n",
       "  'filename': 'avoiding_column_duplication_in_duckdb_joins_with_using.md'},\n",
       " {'datetime': '2024-05-15T11:06:51',\n",
       "  'title': 'Reviewing time_axis_characterisation_and_normalization',\n",
       "  'content': \"It makes it obvious that I really overcomplicate things. In the section 'Measuring Sampling Frequency' I mistake floating point error for real data and derive a method of finding the significant figure of differentation between time points, defining that as the maximum level of precision. Where its obvious from the calculated frequency that two decimal places is sufficient if time is converted from minutes to seconds. The data should in fact be kept in 'seconds'.\",\n",
       "  'tags': '[EDA, time_analysis, frequency, notebooks, log, project/post_build_library, project/etl/api]',\n",
       "  'filename': 'reviewing_time_axis_characterisation_and_normalization.md'},\n",
       " {'datetime': '2024-05-15T14:18:36',\n",
       "  'title': 'Have discovered that sample_metadata is missing 8 samples',\n",
       "  'content': 'Should have 175, only has 163. Furthermore, have discovered that the design of `build_sample_metadata`, makes it even harder to debug than the previous iteration..',\n",
       "  'tags': '[sql, sample_metadata, samples, log, project/post_build_library, project/etl/api]',\n",
       "  'filename': 'have_discovered_that_sample_metadata_is_missing_8_samples.md'},\n",
       " {'datetime': '2024-05-15T16:57:39',\n",
       "  'title': 'its all fucked',\n",
       "  'content': 'Nothing is working, everything is too rigid, too brittle.  I have rewritten the sample_metadata build query, but i cant test it because i cant parametrize the table name. Or cant i? I could write a python routine to replace the table name in the query with another and test on that.',\n",
       "  'tags': '[log, project/post_build_library, project/etl/api]',\n",
       "  'filename': 'its_all_fucked.md'},\n",
       " {'datetime': '2024-05-16T06:36:18',\n",
       "  'title': \"Recycling 'build_sample_metadata'\",\n",
       "  'content': \"The function(?) has been gutted, all logic moved to the query, the transformer now simply reads and executes the query kept in 'create_sample_metadata.sql'.\",\n",
       "  'tags': '[log, project/post_build_library, project/etl/api,]',\n",
       "  'filename': \"recycling_'build_sample_metadata'.md\"},\n",
       " {'datetime': '2024-05-16T10:51:25',\n",
       "  'title': 'Almost finished moving the post_build_library state to sql files',\n",
       "  'content': 'Current plan is to manage the state creation and testing through python, but simple executions of sql queries rather than logic on python side. Everything is in macros so that I can unit test and name the output other things such as \"tbl_test\" etc, as well as limit output to increase speed. Tests should check for correct number of rows, presence of nulls.',\n",
       "  'tags': '[build_library, sql, duckdb, log, project/post_build_library, project/etl/api]',\n",
       "  'filename': 'almost_finished_moving_the_post_build_library_state_to_sql_files.md'},\n",
       " {'datetime': '2024-05-16T10:55:43',\n",
       "  'title': 'state management is hard, and I need to remember why Im doing this - it needs to be reproducible',\n",
       "  'content': 'And that means that all operations to get to the final state must be automated. Macros appear to be the best way of rendering each stage unit-testable. Unfortantely I cannot pass functions to other functions, nor tables as objects. Is what it is.',\n",
       "  'tags': '[log, project/post_build_library, project/etl/api, macros, sql, unit-tests]',\n",
       "  'filename': 'state_management_is_hard,_and_i_need_to_remember_why_im_doing_this_-_it_needs_to_be_reproducible.md'},\n",
       " {'datetime': '2024-05-16T11:17:50',\n",
       "  'title': 'duckdb notes',\n",
       "  'content': '`show tables` shows the tables in the current schema. `show` or `show all tables` shows all tables in all schemas. See https://duckdb.org/docs/guides/meta/list_tables.html.',\n",
       "  'tags': '[duckdb, duckdb-notes,sql, show, tables, all]',\n",
       "  'filename': 'duckdb_notes.md'},\n",
       " {'datetime': '2024-05-16T11:32:48',\n",
       "  'title': 'duckdb notes cont',\n",
       "  'content': 'Moving tables between schemas does not seem directly possible, rather a new table in the destination schema can be created as a copy of the original table, then dropping the original table, if so desired. Not much more verbose than a copy/rename command.',\n",
       "  'tags': '[duckdb, log, project/post_build_library, project/etl/api, sql]',\n",
       "  'filename': 'duckdb_notes_cont.md'},\n",
       " {'datetime': '2024-05-16T11:34:11',\n",
       "  'title': 'Managing sql development and state',\n",
       "  'content': \"Ok, i think i have a working flow. Macros are defined in a macro specific file. A number of macros are combined to produce a final macro, which is then called in a state management file that creates the final table. For example the state designated 'post build library', or 'pbl' is created from macros in 'post_build_library_macros.sql', and the state is created by the queries in 'post_build_library_state_create.sql'. This allows for modular development of the individual actions creating the state, allows them to be unit testable, and seperates the actions from the state creation.\",\n",
       "  'tags': '[duckdb,mres, managing-state, macros, log, project/post_build_library, project/etl/api]',\n",
       "  'filename': 'managing_sql_development_and_state.md'},\n",
       " {'datetime': '2024-05-16T11:59:08',\n",
       "  'title': 'sql macro file ordering',\n",
       "  'content': 'Need to define an easy to read order. as the macros are fully functional, they are little more than wrapping each other like an onion. THus there is a time ordering to their existance, and their order in the file needs to reflect that for easy reading. From here I will order them in reverse time order, with the last one first.',\n",
       "  'tags': '[sql, development, functional-programming, log, project/post_build_library, project/etl/api]',\n",
       "  'filename': 'sql_macro_file_ordering.md'},\n",
       " {'datetime': '2024-05-16T12:29:29',\n",
       "  'title': 'Testing the new state creation',\n",
       "  'content': \"The state creation flow mentioned earlier is verified, however I now need a method of testing it. There is sqllogictest, which is recommended by [duckdb](https://duckdb.org/docs/dev/sqllogictest/intro.html) but I cant find a Python intregration, and as my other tests are there I am loathe to change now. Python based unit tests calling the macros in the respective files and testing the output is probably the way to go, at least at this point. Simply put, and in the interest of saving time, just test the output of the 'create' macros. Just use the column count and estimated size from duckdb_tables\",\n",
       "  'tags': '[sql, mres, testing, log, project/post_build_library, project/etl/api]',\n",
       "  'filename': 'testing_the_new_state_creation.md'},\n",
       " {'datetime': '2024-05-16T16:04:07',\n",
       "  'title': 'macro integration',\n",
       "  'content': 'it doesnt appear that macros are optimized, they seem to execute in entirety before moving on to the next statement. This means that they are not as useful as previously thought, because for example expensive queries in macros remain unoptimized. The search for testing options continues..',\n",
       "  'tags': '[sql, duckdb, macros, testing, log, macro, queries, sql, project/etl/api, project/post_build_library]',\n",
       "  'filename': 'macro_integration.md'},\n",
       " {'datetime': '2024-05-16T23:17:35',\n",
       "  'title': 'Swapping macros for CTEs',\n",
       "  'content': \"So yeah, doesnt look like macros are the way foreward that I expected. I think we're gna have to do away with the idea of unit testing, and swap to CTEs, as they may be better optimized. Lets test it by recreating the code already written, but with ctes rather than macros. as you can reference ctes in other ctes, the functional nature of the code remains.\",\n",
       "  'tags': '[cte, log, code, macros, sql, project/etl/api, project/post_build_library]',\n",
       "  'filename': 'swapping_macros_for_ctes.md'},\n",
       " {'datetime': '2024-05-16T23:57:04',\n",
       "  'title': 'Observations from CTE implementation',\n",
       "  'content': 'Running the queries up until the melt - before the sample metadata join to add wine names - takes 47 seconds (not writing a table). The macro version runs for > 1 minute.',\n",
       "  'tags': '[sql, sql-macros, sql-cte, sql-unpivot, log, project/etl/api, project/post_build_library]',\n",
       "  'filename': 'observations_from_cte_implementation.md'},\n",
       " {'datetime': '2024-05-17T09:24:31',\n",
       "  'title': 'Where to develop sql files',\n",
       "  'content': \"Am struggling to find a good flow. vscode has one extension, 'SQLTools', but it doesnt support the current release of DuckDB. DBeaver is ok but the ui is gross, the editing and keybinds are weird, and its handling of sql scripts is not in file? Harlequin was actually really good until i discovered that it doesnt have an autosave feature, and one crash undid 10 minutes of work. Neovim doesnt handle sql by default and i dont want to spend the time on that. And finally, redirecting sql files directly to duckdb results in poor formatting in the native terminal. One thing I hadnt considered, and considering I am all about TDD, was writing in sql files for execution in the python test file. we'll go with that today, so that we have an end.\",\n",
       "  'tags': '[log, project/etl/api, editors, TDD, sql, duckdb, neovim, harlequin-sql, python, dbeaver, terminal]',\n",
       "  'filename': 'where_to_develop_sql_files.md'},\n",
       " {'datetime': '2024-05-17T10:32:12',\n",
       "  'title': 'finalizing development flow',\n",
       "  'content': \"TDD in python works. furthermore the cte approach with cs long seems much better than macro. Finally, now that we've played a bit more with cross language and platform development, its clear that I need to be creating packages as command line executable first.\",\n",
       "  'tags': '[log, chromatogram_spectra_long, macro, packages, cli, projceet/etl/api, project/post_build_library, tdd, python, sql, cte]',\n",
       "  'filename': 'finalizing_development_flow.md'},\n",
       " {'datetime': '2024-05-17T10:34:21',\n",
       "  'title': 'whats next',\n",
       "  'content': 'I now need to finish testing the cs long query, and then write a script to get to the pbl state, from build library state. I also need to come up with better names for this stuff.',\n",
       "  'tags': '[tdd, mres, sql, python, build_library, project/etl/api, project/post_build_library, log, chromatogram_spectra_long]',\n",
       "  'filename': 'whats_next.md'},\n",
       " {'datetime': '2024-05-17T10:39:39',\n",
       "  'title': 'how to organise the state update',\n",
       "  'content': \"controling the flow of time from 'build library' to 'post build library' is difficult. The individual queries should be seperate to allow unit testing, for example 'sample_metadata' table creation shouldnt depend on 'chromatogram_spectra_long' creation, but how to organise it? This is where python comes in (or another scripting language). I can turn each of the calls into a python object, then call them sequentially, with tests if necessary.\",\n",
       "  'tags': '[python, automation, sql, build_library, project/etl/api, project/post_build_libray, log]',\n",
       "  'filename': 'how_to_organise_the_state_update.md'},\n",
       " {'datetime': '2024-05-17T11:49:40',\n",
       "  'title': 'modifying query strings for testing',\n",
       "  'content': 'Using simple string substitution to modify queries to make them test-friendly seems to work pretty well. For example modifying a tables name to point to a python run-time relation object. ',\n",
       "  'tags': '[log, project/post_build_library, project/etl/api, tdd, sql, duckdb, testing]',\n",
       "  'filename': 'modifying_query_strings_for_testing.md'},\n",
       " {'datetime': '2024-05-17T11:50:43',\n",
       "  'title': 'bringing it all together',\n",
       "  'content': 'The state-setting queries are written and tested, but now I need to establish the flow. What does it need to do? 1. write chromatogram_spectra_long, 2. write sample_metadata, 3. join the wine and sample_num to chromatogram_spectra_long from sample_metadata. I believe we can create relation objects from a selection query in a .sql file then call the table creation with inline sql. Finally, we would need a simple outcome verification function - have these tables been created, do they match expectation. That module could then be called from the command line.',\n",
       "  'tags': '[log, chromatogram_spectra_long, sample_metadata, tdd, build_library, sql, duckdb, mres, python, project/etl/api, project/post_build_library]',\n",
       "  'filename': 'bringing_it_all_together.md'},\n",
       " {'datetime': '2024-05-17T13:57:25',\n",
       "  'title': 'where ctes go in the query',\n",
       "  'content': \"ctes are part of the select statement, and as such for actions such as 'create table x as ..', the 'create table as..' component goes first, THEN the cte declaration, then the selection.\",\n",
       "  'tags': '[syntax, sql, cte, sql, create, table]',\n",
       "  'filename': 'where_ctes_go_in_the_query.md'},\n",
       " {'datetime': '2024-05-17T14:30:31',\n",
       "  'title': 'updated test flow',\n",
       "  'content': 'Use transactions, drop tables / columns, etc, call the script to make the  change, observe if the change is as expected. then we dont have to worry about modifying the script.',\n",
       "  'tags': '[tdd, sql, mres, sql-transactions, sql, transaction, drop, table, column, script]',\n",
       "  'filename': 'updated_test_flow.md'},\n",
       " {'datetime': '2024-05-17T14:31:42',\n",
       "  'title': 'friday roundup',\n",
       "  'content': \"im out of time, i can do a little bit more after work, but really i wont be able get back to this till monday. WE need to finish testing / devving the pbl script, but we've got the actions now, and we can unit test easily. once thats done, finish cleaning up the notebooks - do that by summarizing the HELL out of them, they are way too verbose to be left as they are. for example the time offset analysis is a 1 liner. doesnt need all of the other dross. have the query to express the distribution as well, but thats it. Reduce each to a paragraph and associated query.\",\n",
       "  'tags': '[log, mres, friday-roundup, notebooks, preprocessing, tdd, time_offset, project/etl/api, project/post_build_library, notebooks]',\n",
       "  'filename': 'friday_roundup.md'},\n",
       " {'datetime': '2024-05-17T15:44:57',\n",
       "  'title': 'perfecting the test database',\n",
       "  'content': \"While finishing the testing for the pbl state creation function, I have run into an unexpected stall while trying to add the wine and sample_num columns to chromatogram_spectrum_long. It is not clear why this is happening, and the development cycle is too long. I dont need to test on every single sample, as queries are taking too long to run. As it currently stands, there are about a million rows per sample. reducing the test set to 10 samples will still include 10 million rows. A method of filtering would be to join on sample_metadata after taking a sample from it. TODO: create the sample set, get a list of 10 samples so that you have deterministic outcome, create the queries in 'create_test_schema.sql'.\",\n",
       "  'tags': '[log, project/etl/api, project/post_build_library, sql, chromatogram_spectra_long, wine, sample_num, column, todo]',\n",
       "  'filename': 'perfecting_the_test_database.md'},\n",
       " {'datetime': '2024-05-17T23:51:36',\n",
       "  'title': 'a step back',\n",
       "  'content': \"I dont think I checked the query sequence with a simple limit added. Try that before introducing another table. Also, try the table replacement vs. the update, it might be that the update is more taxing. For the second point - it takes 12 seconds to execute the 'alter table' query, and 24 for the 'as join' query. Altering the table as opposed to completely replacing it takes half the time. The motivation for creating the tes ttable is that there is no select in the query so I cannot limit(?) the query size. But i can limit the size of chromatogram_spectra_long in its creation query. that is the easiest way. Or you could redefine the object structure to output a list of queries, then use execute many rather than iterating.\",\n",
       "  'tags': '[sql, log, project/etl/api, project/post_build_library, sql, query, update, join]',\n",
       "  'filename': 'a_step_back.md'},\n",
       " {'datetime': '2024-05-18T00:13:11',\n",
       "  'title': 'confirming earlier results',\n",
       "  'content': 'confirmed, something odd is happening with the alter table query. However when limiting the table to 10 rows it functions as expected. 1,000,000 rows works as expected, 13 sec execution time. At 10 million rows the slow query behavior reappears, but finishes after 2 minutes 15 seconds. What happens if we use the join approach instead? It takes 1 minute and 15 seconds on 10 million rows, and.. 4 minutes to complete the query on the total dataset. The other option to test is whether its faster on the wide vesrion of the table, and whether it slows down the unpivot. so, todos: 1. add index and try. 2. add names and sample_num before unpivot.',\n",
       "  'tags': '[log, project/etl/api, project/post_build_library, alter, table, sql, unpivot, index]',\n",
       "  'filename': 'confirming_earlier_results.md'},\n",
       " {'datetime': '2024-05-18T00:35:07',\n",
       "  'title': 'benefits of indexes',\n",
       "  'content': 'Apparently indexes can help speed up operations such as joins. Should add some to the tables and see if anything changes. Need to add an index to chromatogram_spectra_long and sample_metadata, both being id.',\n",
       "  'tags': '[sql, index, joins, mres, project/post_build_library, log, project/etl/api, build_library, chromatogram_spectra_long, sample_metadata]',\n",
       "  'filename': 'benefits_of_indexes.md'},\n",
       " {'datetime': '2024-05-18T01:16:07',\n",
       "  'title': 'results of adding the index',\n",
       "  'content': 'Nothing changed for the join. and the alter table? I cancelled it after 7 minutes.',\n",
       "  'tags': '[sql, index, duckdb, mres, join, alter, table, log, project/etl/api, project/post_build_library]',\n",
       "  'filename': 'results_of_adding_the_index.md'},\n",
       " {'datetime': '2024-05-18T02:29:21',\n",
       "  'title': 'A Result',\n",
       "  'content': \"We're now encountering OOM errors while executing the queries in series.\",\n",
       "  'tags': '[project/etl/api, project/post_build_library, duckdb, query, python]',\n",
       "  'filename': 'a_result.md'},\n",
       " {'datetime': '2024-05-18T10:31:58',\n",
       "  'title': 'todo',\n",
       "  'content': 'fiinish fixing build_library_oop tests, commit them and everything else in pbl.',\n",
       "  'tags': '[log, project/etl/api, project/build_library/post_build_library]',\n",
       "  'filename': 'todo.md'},\n",
       " {'datetime': '2024-05-19T00:04:57',\n",
       "  'title': 'a way forward',\n",
       "  'content': 'A eureka moment. Data analysis design and develpment should be functional first. we are for the most part interested in going from state A to state B through a series of transformations, usually of a 1 dimensional array. The properties of the current state are not actually interesting, only the final state, therefore a functional approach removes a lot of overhead. Therefore, a TDD appraoch coupled with functional-first, possibly with a OOP API is how I will proceed. This makes testing easy, as state is very easy to manage, and allows us to wrap the top level functions in whatever structure we need for interfacing with other libraries, such as sci-kit learn transformers. Thus, a final paradigm has emerged. top level transformer functions first, class based apis if necessary, with intermediate state managed by polars pipes. Also, as much duckdb as possible. Or something like that.',\n",
       "  'tags': '[log, project/deconvolution, tdd, testing, oop, sklearn, programming, polars, duckdb]',\n",
       "  'filename': 'a_way_forward.md'},\n",
       " {'datetime': '2024-05-20T08:46:40',\n",
       "  'title': 'Finishing deconvolution and integration',\n",
       "  'content': \"Time to bring it all together. To do so, I need to migrate my code from hplc-py to wine_analysis_hplc_uv. Then I need to rebuild the pipe and test it with a sample dataset, then with real data. To simplify things, I want to discard scikit learn for now, and focus on honing the pipeline steps as pure functions, then wrap the pure functions in the scikit learn api downtrack. So by 2pm we'll have some results. Therefore, timeframe: 9 - 10: complete migration, set up environment. 10 - 12: run sampleset, debug. 12 - 1: running real dataset. 1 - 2: clean up. It seems an unrealistically rapid pace, but lets see how we go. Specific notes will be at [Deconvolution Migration](../README.md#deconvolution-migration).\",\n",
       "  'tags': '[package-management, deconvolution,signal-processing, sklearn, deconvolution, project/hplc_py_migration, scikit, ]',\n",
       "  'filename': 'finishing_deconvolution_and_integration.md'},\n",
       " {'datetime': '2024-05-21T21:44:16',\n",
       "  'title': 'back on track',\n",
       "  'content': \"Ok, package seems to be healthy again? and we're now running 3.12. I guess we'll have to run tests to find out.\",\n",
       "  'tags': '[log, project/migrating_to_python_312]',\n",
       "  'filename': 'back_on_track.md'},\n",
       " {'datetime': '2024-05-28T23:24:39',\n",
       "  'title': 'is normal distribution scale equal to peak width? As per title',\n",
       "  'content': 'Im not sure about this. Cant find any informatino on the net, so best to just test it. Easiest way will be to generate a pdf then measure it and compare. The answer? related but not equal. The literature is in agreement. The scale is a parameter that describes the spread of the distribution across x, but as the AUC is always 1, increasing scale decreases height.',\n",
       "  'tags': '[normal_distribution, log, project/convoluted_skewnorm_generation, auc, scale, width]',\n",
       "  'filename': 'is_normal_distribution_scale_equal_to_peak_width?_as_per_title.md'},\n",
       " {'datetime': '2024-05-28T23:36:19',\n",
       "  'title': 'observations of the parameters of the normal distribution',\n",
       "  'content': \"A 'normal' looking distribution has a scale of 10% of the range of x, and location half the range of x. Increasing the scale reduces the y maxima, and vice versa, but it appears that the area is always the same, equal to 1 (before transformation). Hence height and scale are always working in inverse. According to [statisticshowto](https://www.statisticshowto.com/scale-parameter/), in the standard normal distribution, scale is equal to standard deviation, or half the peak width at a height somewhere vaguely above the half height point, depending on the scale. As we recall, the standard deviation is equal to the square root of the  sum of the mean residuals and observations divided by the number of observations. Thus it is clear that the use of peak width measured at a relative height of 1 is not attempting to estimate distribution scale at all, but it is simply an easy means of providing an initial guess. One which I would say is flawed, but enough to produce results. Finding a better means of estimating scale may be a route to increasing the performance.\",\n",
       "  'tags': '[log, normal_distribution, skew_norm, deconvolution, peak_width, peak_height, peak_mapping ,mres, scale, height, residuals, project/deconvolution, area, project/deconvolution, project/convoluted_skewnorm_generation]',\n",
       "  'filename': 'observations_of_the_parameters_of_the_normal_distribution.md'},\n",
       " {'datetime': '2024-05-29T09:54:56',\n",
       "  'title': 'Role of Scale in the Skew-Normal Distribution',\n",
       "  'content': 'But what is the role of scale in the skew-normal distribution? [wikipedia](https://en.wikipedia.org/wiki/Skew_normal_distribution) it appears that the skew-normal only introduces the skew as a coefficient to the x term, thus the scale role should be the same, modified by skew. So we can assume that half the width at half height is a good approximation of the scale of the peak. Ergo, that measurement should allow an approximation of the peak, decreasing in efficacy as the skew increases.',\n",
       "  'tags': '[log, skew_norm, scale, width, height, peak, project/deconvolution, project/convoluted_skewnorm_generation]',\n",
       "  'filename': 'role_of_scale_in_the_skew-normal_distribution.md'},\n",
       " {'datetime': '2024-05-29T10:01:49',\n",
       "  'title': 'Observing the Effect of Skew on Peak Width Measurements Through Topographic Methods',\n",
       "  'content': 'I should answer the following questions - how does measured peak width change as skew increases? How does model fit change as skew increases? And what about changing the width measurement height, what effects does that have?',\n",
       "  'tags': '[log, project/deconvolution, project/convoluted_skewnorm_generation, skew, width, topography, model, height]',\n",
       "  'filename': 'observing_the_effect_of_skew_on_peak_width_measurements_through_topographic_methods.md'},\n",
       " {'datetime': '2024-05-29T10:07:36',\n",
       "  'title': 'How does Measured Peak Width Change with Skew? To answer this, we would need a function to generate the pdf, then measure the peak width of the pdf',\n",
       "  'content': \"The underlying factor is this - measured peak width is dependent on peak prominence, which works for isolated peaks but suffers as peaks become convoluted, because the 'center' of the peak as measured becomes higher and higher (need to confirm this).\",\n",
       "  'tags': '[peak_width, project/deconvolution, project/convoluted_skewnorm_generation, peak, prominence, log]',\n",
       "  'filename': 'how_does_measured_peak_width_change_with_skew?_to_answer_this,_we_would_need_a_function_to_generate_the_pdf,_then_measure_the_peak_width_of_the_pdf.md'},\n",
       " {'datetime': '2024-05-29T09:39:30',\n",
       "  'title': 'musing on estimation of scale through peak width',\n",
       "  'content': 'Cremerlab does not emphasise how precarious that assumption is, as the more convoluted the peak, the less its prominence, and thus the relative height has more weight when compared to an unconvoluted signal. Is that true? Again, I need a means of generating signals in order to explore this. Perhaps it would be beter to use sinosoidal waves.',\n",
       "  'tags': '[project/deconvolution, cremerlab, peak_scale, peak_width, prominance, relative_height, weight, deconvolution, signals, sinosoidal_waves, log, project/convoluted_skewnorm_generation,]',\n",
       "  'filename': 'musing_on_estimation_of_scale_through_peak_width.md'},\n",
       " {'datetime': '2024-05-29T15:59:13',\n",
       "  'title': 'absurd day',\n",
       "  'content': \"So I tried to write a short piece on the skew normal distribution, as per the notes above. But where to put the notes? I decided to structure the notes directory as per the tests, mirroring the package. Fine and dandy, but to then enact that structure I had to clean up the notes already present, also fine, but they needed cleaning up a little bit first.. right? and while im going, I might as well delete any unnecessary ones. Oh and why am I in both ipynb and qmd files? Best convert everything to qmd on the way.. wait how do I structure them now, as when rendering the qmd files the directories get cluttered with the html files.. Oh and these dont actually run so I need to fix them to make sure that they have everything that.. wait the production database is not up to date? Ok better run the post build-library pipeline.. wait theres no docs, a bunch of todos, and no clear method of running it? Fucking excellent. But at least the tests were completed, we can execute those as we refactor then see if itll work in prod. OH WAIT, PYTEST CANT COLLECT THE TESTS BECAUSE OF ERORS THROUGHOUT OTHER TESTS!! So now we're fixing random tests so we can refactor the pbl directory to provide a UI. So i can get the prod db up to date so I can finish refactoring documents from 9 months ago so I can finish organising my notes so I can write new notes about the norm distribution so I can finish rewriting the deconvolution module so I can deconvolute my signals so I can bin the peaks so I can compare across the samples so I can write a simple EDA report. New rule - commit anything by 5, on a seperate branch if need be. In fact, it should always be on a seperate branch, manage your dependencies better, no classes, and never do anything like this ever again.\",\n",
       "  'tags': '[project/deconvolution, skew_normal_distribution, roundup, project_management, quarto, tests, pbl, python, log, project/convoluted_skewnorm_generation]',\n",
       "  'filename': 'absurd_day.md'},\n",
       " {'datetime': '2024-05-29T16:34:12',\n",
       "  'title': 'post_build_library in production',\n",
       "  'content': 'Have for the first time successfully execute `pbl_state_creation.update_library` on the production database. A milestone for sure. Have not written it as a standalone script, as its a simple matter to create the connection object and call the function on it.',\n",
       "  'tags': '[project/etl/api, project/post_build_library, mres, duckdb, log]',\n",
       "  'filename': 'post_build_library_in_production.md'},\n",
       " {'datetime': '2024-05-30T11:29:12',\n",
       "  'title': 'notes on killing processes',\n",
       "  'content': 'Quarto is very much still in beta, and I am running into a problem where if it fails to render a document half way through, and has a currently open duckdb connection, it is not closing the connection, holding the lock in place. While I could context management to handle the connections, that kills a sense of flow, the whole point of the document. So I needed a means of killing the running process without restarting the computer. In comes bash `kill`, which as it sounds, kills running processes according to their PID, usefully provided by duckdb on `IOException` error.',\n",
       "  'tags': '[log, project/note_cleanup,bash ,duckdb, quarto, error, shell, process, PID, kill]',\n",
       "  'filename': 'notes_on_killing_processes.md'},\n",
       " {'datetime': '2024-05-30T12:29:50',\n",
       "  'title': 'rewriting dtw notes',\n",
       "  'content': 'The dtw notes are a cesspit of over-abstracted functions, missing data files, pandas multiindex madness and missing documentation. Its going to take days to fix, and ive got hours. even less than that. Tecnically ive got an hour and a half to finish the thesis. lol. it can remain as it is, and will be left to the very end.',\n",
       "  'tags': '[project/note_cleanup, dtw, log, thesis,pandas]',\n",
       "  'filename': 'rewriting_dtw_notes.md'},\n",
       " {'datetime': '2024-05-30T12:35:30',\n",
       "  'title': 'Continuing on note organisation',\n",
       "  'content': 'The rewriting and organising of existing notes is going well, the mirroring of the package structure in the notes directory is promising, and while the conversion to qmd has not been without hiccups, overall i prefer the format to ipynb, plain text is always preferable. One problem is code formatting, but it is a very late stage concern.',\n",
       "  'tags': '[log, project/note_cleanup, project_structure, quarto, ipython_notebooks, code, formatting, mres]',\n",
       "  'filename': 'continuing_on_note_organisation.md'},\n",
       " {'datetime': '2024-05-31T03:51:10',\n",
       "  'title': 'operations on long tables, or 175x10^6 rows is too much',\n",
       "  'content': \"As title. Cant do shit, and its wasting all my time. My previous approach was to apply the transformations on selection, adding layers and layers in order to reach the current *state*. it's bullshit though. There has to be an approach that works. What I need is a community that can point me in the right direction. Also how do we make the query work? we first figure out what arithmatic operations DO work, and whether we need to create a temporary table in between operations. Or. Another table. Create a cs table at 256 nm for further operations. At this point in time the deconvolution precludes a 3d dataset, at least until the operation can be expanded to 3 dimensions.\",\n",
       "  'tags': '[log, project/etl/api, sql, chromatogram_spectra, deconvolution, data_3d]',\n",
       "  'filename': 'operations_on_long_tables,_or_175x10^6_rows_is_too_much.md'},\n",
       " {'datetime': '2024-06-04T16:54:36',\n",
       "  'title': 'tuesday roundup',\n",
       "  'content': \"I've been busy. When I last touched base I was trying to rewrite the time offset correction as a dataset wide arithmatic query that was refusing to complete within an acceptable time. Since then I realised that I was expecting too much of the database and my computer, and have since decided to do the following: TODO: denormalise the time column into a table of mins with sample id and row idx as key. Replace with the row idx in 'cs_long'. Then time based operations can be completed on the 1 time column that is the same for evey wavelength in the sample, rather than trying to compute the same thing for every wavelength in every sample, reducing the size of the operation by the number of wavelengths in each sample, approximately (600-192)/2 * 175 = 35,700, or reducing it by a factor of 6, which will certainly be a managable operation. In the meantime I've been cleaning up my notes and old modules with the intention of bringing those spaces back to life, i.e. have a core signal processing module that contains all the methods required, and get all my notes running again so I can use them to publish. Finally, aggregating all my notes together so that topics emerge, from the topics, articles, chapters etc. Ergo, the package structure needs to be perfectly reflected in the tests structure, and the notes structure, and also the zotero collection structure. Thus the content exists across these four platforms, and they will in essence keep each other in check. Need a note for the function, tests for the function, references for the note. the only thing im missing now is to A: figure out a better view for zotero collections, as you are able to store items within a collection, but subcollections are treated differently - want a file-explorer type display, subcollections and items treated as objects of their containing collection, and b. tests for notebooks to ensure that modifications dont break them.\",\n",
       "  'tags': '[log, roundup, offest, sql, database, denormalization, project/time_offset_correction, project/time_denormalization, project/note_cleanup, zotero, collections, project/zotero_cleanup]',\n",
       "  'filename': 'tuesday_roundup.md'},\n",
       " {'datetime': '2024-06-05T11:04:14',\n",
       "  'title': 'Note taking',\n",
       "  'content': 'Again trying come up with a frictionless note taking process. Atm I am forming an outline from a source and restructuring it as a structure becomes clear. Furthermore, in lieu of being able to form inline links, creating seperate qmd documents and then cross-referencing them seems to be the way to go, but like everything else in quarto its.. janky. I have moved to using author name, short title and year as cite key to give more context, btw.',\n",
       "  'tags': '[quarto, thesis, note-taking, qmd]',\n",
       "  'filename': 'note_taking.md'},\n",
       " {'datetime': '2024-06-05T11:05:39',\n",
       "  'title': 'quarto cross referencing and yaml title field',\n",
       "  'content': 'Cant cross reference the yaml title if using that, and the numbering includes the title and then the scond title if you use that instead, so if u want to cross reference, dont use the yaml field.',\n",
       "  'tags': '[cross_referencing, yaml, title, quarto]',\n",
       "  'filename': 'quarto_cross_referencing_and_yaml_title_field.md'},\n",
       " {'datetime': '2024-06-05T13:26:00',\n",
       "  'title': 'quarto reference figures and code cells',\n",
       "  'content': 'code cell output is treated like figures, who have a comment pipe syntax for their relationship with the document and referencing, depending on the syntax of the code in question. For python it is \"#|\", for mermaid \"%%|\". Quarto scans the code for the comment pipes then acts on the command. For alignment, use `#|layout-align`, for cross-referencing, first label with `#|label: \\'a-label\\'`. See [diagrams](https://quarto.org/docs/authoring/diagrams.html), [cross-references](https://quarto.org/docs/authoring/cross-references.html), [panel-layout](https://quarto.org/docs/reference/cells/cells-jupyter.html#panel-layout), [diagrams, cross-references](https://quarto.org/docs/authoring/diagrams.html#cross-references). Captions are added by specifying `fig-cap`, with a heirarchical `sub-cap` option [subcaptions](https://quarto.rocks/authoring/figures#subcaptions), [figures, subcaptions](https://quarto.org/docs/authoring/figures.html#subcaptions). Note that captions are left aligned by default with no option to change. There are css hacks that promise to modify this, but I have not managed to make it work [alignment of figure captions](https://github.com/quarto-dev/quarto-cli/discussions/7003#discussioncomment-7112704), [center-caption-in-quarto](https://stackoverflow.com/questions/76404758/center-caption-in-quarto).',\n",
       "  'tags': '[quarto, references, figures, code_cells, mermaid, comments, layout, cross-referencing, captions, css]',\n",
       "  'filename': 'quarto_reference_figures_and_code_cells.md'},\n",
       " {'datetime': '2024-06-07T08:55:32',\n",
       "  'title': 'recycling hplc-py',\n",
       "  'content': 'TODO: digest the resolution enhancement notebooks. Then digest the methods.',\n",
       "  'tags': '[hplc-py, project/deconvolution, notebooks]',\n",
       "  'filename': 'recycling_hplc-py.md'},\n",
       " {'datetime': '2024-06-07T08:57:17',\n",
       "  'title': 'another attempt at proof of concept',\n",
       "  'content': \"Need to develop a rudimentary deconv pipeline THEN fine tune it. As I've experienced for literally the last 6 months though, its not easy, there are too many moving parts. The last attempt (in the hplc-py fork) failed because I tried to be too fancy, and the individual components were not encapsulated enough, which raised the complexity to unmangable levels. A clean, pure function and module based approach is in my opinion the best approach for development, then wrapping everything in a class IF NECESSARY.\",\n",
       "  'tags': '[project/deconvolution, encapsulation, functions, class]',\n",
       "  'filename': 'another_attempt_at_proof_of_concept.md'},\n",
       " {'datetime': '2024-06-07T09:01:59',\n",
       "  'title': 'moving forward',\n",
       "  'content': 'Researching dsp is my best bet at finishing this. But the code contains the results, and the structure, they need to grow in tandem. The notes and the code. The notes, the code and the tests. However, my eyes are starting to give me trouble, as they do every time I start any serious reading. Working on the notes and code alternatively seems to be a good approach, as my eyes dont suffer as much when coding.',\n",
       "  'tags': '[project/deconvolution, digital_signal_processing, research, eyes, coding]',\n",
       "  'filename': 'moving_forward.md'},\n",
       " {'datetime': '2024-06-12T11:49:09',\n",
       "  'title': \"Merging 'moving_notes_writing_deconv' With Main\",\n",
       "  'content': \"As title, the deconvolution project has been shelved whiel I finish chapter 1. in the interest of avoiding muddying the history, i'll merge to main now.\",\n",
       "  'tags': '[project/deconvolution, chapter_1, project_management]',\n",
       "  'filename': \"merging_'moving_notes_writing_deconv'_with_main.md\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add title field as lowered and underscore seperated titles, stripped and illegal characters removed\n",
    "\n",
    "from wine_analysis_hplc_uv import definitions\n",
    "\n",
    "\n",
    "def add_filenames(notes: list[dict[str, str]]) -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    add a 'file_name' pair whose value is created from the 'title' string, cleaned for use as a file name. It is validated by testing whether the new file can be written.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        temp_dir: Path = Path(definitions.ROOT_DIR).parent / \"test_valid_names\"\n",
    "\n",
    "        temp_dir.mkdir()\n",
    "\n",
    "        for note in notes:\n",
    "            title = note[\"title\"]\n",
    "\n",
    "            # clean\n",
    "            name = title.lower().strip()\n",
    "\n",
    "            # remove any trailing punctuation if present\n",
    "            if name[-1] in [\"?\", \",\", \".\"]:\n",
    "                name = name[:-1]\n",
    "\n",
    "            # replace spaces\n",
    "            name = name.replace(\" \", \"_\")\n",
    "\n",
    "            # add \".md\"\n",
    "            name = name + \".md\"\n",
    "\n",
    "            # test the new file\n",
    "\n",
    "            temp_outpath = temp_dir / name\n",
    "\n",
    "            try:\n",
    "                temp_outpath.touch(exist_ok=False)\n",
    "            except OSError as e:\n",
    "                e.add_note(f\"file name potentially  invalid: {name}\")\n",
    "                raise e\n",
    "            finally:\n",
    "                temp_outpath.unlink()\n",
    "                note[\"filename\"] = name\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        temp_dir.rmdir()\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "add_filenames(notes=decomp_notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tags(notes):\n",
    "    \"\"\"\n",
    "    Convert the tags from strings to lists of strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the tags\n",
    "\n",
    "    for note in notes:\n",
    "        tags = note[\"tags\"].strip()\n",
    "        # remove brackets\n",
    "        tags_without_brackets = tags[1:-1].strip()\n",
    "\n",
    "        # if trailing comma, remove\n",
    "\n",
    "        if tags_without_brackets[-1] == \",\":\n",
    "            tags_without_brackets = tags_without_brackets[:-1]\n",
    "\n",
    "        cleaned_tags = [tag.strip() for tag in tags_without_brackets.rsplit(\",\")]\n",
    "\n",
    "        for tag in cleaned_tags:\n",
    "            if \" \" in tag:\n",
    "                raise ValueError(\n",
    "                    f\"something went wrong when parsing {note['title']}, space detected..\"\n",
    "                )\n",
    "\n",
    "        note[\"cleaned_tags\"] = cleaned_tags\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "decomp_notes = parse_tags(notes=decomp_notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'frontmatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/jonathan/mres_thesis/wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/notes/decomposing_devnotes.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jonathan/mres_thesis/wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/notes/decomposing_devnotes.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfrontmatter\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathan/mres_thesis/wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/notes/decomposing_devnotes.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m frontmatter_objs \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathan/mres_thesis/wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/notes/decomposing_devnotes.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m note \u001b[39min\u001b[39;00m decomp_notes:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'frontmatter'"
     ]
    }
   ],
   "source": [
    "import frontmatter\n",
    "\n",
    "frontmatter_objs = []\n",
    "\n",
    "for note in decomp_notes:\n",
    "    frontmatter.Post()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-analysis-hplc-uv-F-SbhWjO-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
